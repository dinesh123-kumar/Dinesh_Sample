# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PvzPGSJJqf8_8jrsn8yPHnOEoyKB-L26
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv("/content/Churn_Modelling.csv")
df.head()

# Data preprocessing

df=df.drop(["RowNumber","CustomerId","Surname"],axis='columns')
df.head()

# Checking Missing Values
print(df.isnull().sum())

df.dtypes

# print(df['Geography'].unique())
# print(df['Gender'].unique())

for col in df.columns:
    if df[col].dtype == 'object':
       unique_values = df[col].unique()
       print(f"Unique values in column '{col}': {unique_values}")

df

df.Gender.replace({'Male':1,'Female':0},inplace=True)
df.head()

df=pd.get_dummies(data=df,columns=['Geography'])
df.head()
df.dtypes

columns_bool=['Geography_France','Geography_Germany','Geography_Spain']
for col in columns_bool:
    df[col].replace({True:1,False:0},inplace=True)
df.head()
df.dtypes

df.describe()

correlation_matrix= df.corr()
correlation_matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

#Exploratory data analysis
#churn exited plot
plt.figure(figsize=(8, 6))
sns.countplot(x='Exited', data=df)
plt.title('Distribution of Churn (Exited)')
plt.show()

#tenure vs no of exited
tenure_yes= df[df['Exited']==1]['Tenure']
tenure_no= df[df['Exited']==0]['Tenure']
plt.xlabel('Tenure')
plt.ylabel('No of Customers')
plt.title('Customer Churn Prediction visualization')
plt.hist([tenure_yes, tenure_no],color=['Green','Red'], label=['Churn=Yes', 'Churn=No'])
plt.legend()

creditscore_yes= df[df['Exited']==1]['CreditScore']
creditscore_no= df[df['Exited']==0]['CreditScore']
plt.xlabel('CreditScore')
plt.ylabel('No of Customers')
plt.title('Customer Churn Prediction visualization')
plt.hist([creditscore_yes, creditscore_no],color=['Green','Red'] ,label=['Churn=Yes', 'Churn=No'])
plt.legend()

balance_yes= df[df['Exited']==1]['Balance']
balance_no= df[df['Exited']==0]['Balance']
plt.xlabel('Balance')
plt.ylabel('No of Customers')
plt.title('Customer Churn Prediction visualization')
plt.hist([balance_yes, balance_no],color=['Green','Red'],label=['Churn=Yes', 'Churn=No'])
plt.legend()

df.head()

#scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']] = scaler.fit_transform(df[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']])
df.head()

# prompt: Select and Justify Machine Learning Algorithms for Churn Prediction

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Define features (X) and target variable (y)
df
X = df.drop('Exited', axis=1)
y = df['Exited']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Logistic Regression
logreg_model = LogisticRegression()
logreg_model.fit(X_train, y_train)
logreg_pred = logreg_model.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, logreg_pred))
print(classification_report(y_test, logreg_pred))
cm = confusion_matrix(y_test, logreg_pred)
sns.heatmap(cm,annot=True,fmt='d')
plt.show()





# Justification:

# * Logistic Regression:  A good baseline model.  It's interpretable and efficient,
#   but might not capture complex relationships in the data.  Its performance will help establish a benchmark.

# * Random Forest:  A powerful ensemble method that often performs well on various datasets.
#   It can handle non-linear relationships and is less prone to overfitting than individual decision trees.
#   It's a strong candidate for churn prediction due to its ability to capture interactions among features.

# * Support Vector Machine (SVM):  Effective for high-dimensional data and can find complex decision boundaries.
#   However, SVM's performance can be sensitive to parameter tuning, and it might be computationally expensive
#   for very large datasets.  We include it to explore non-linear relationships within the data.


# Further steps:
# * Hyperparameter tuning for each model (especially Random Forest and SVM)
# * Feature engineering (create new features that might improve prediction accuracy)
# * Evaluate using more metrics (precision, recall, F1-score, AUC-ROC)
# * Cross-validation to get a more robust performance estimate
# * Consider other algorithms like Gradient Boosting Machines (GBM) or XGBoost.

# 2. Random Forest
rf_model = RandomForestClassifier(random_state=100)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
print("\nRandom Forest Accuracy:", accuracy_score(y_test, rf_pred))
print(classification_report(y_test, rf_pred))
cm=confusion_matrix(y_test,rf_pred)
sns.heatmap(cm,annot=True,fmt='d')
plt.show()

# 3. Support Vector Machine (SVM)
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)
svm_pred = svm_model.predict(X_test)
print("\nSVM Accuracy:", accuracy_score(y_test, svm_pred))
print(classification_report(y_test, svm_pred))
cm=confusion_matrix(y_test,svm_pred)
sns.heatmap(cm,annot=True,fmt='d')
plt.show()
#

import tensorflow as tf
from tensorflow import keras

# Define the model
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32) # Adjust epochs and batch_size as needed

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")


#Make predictions
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5) # Convert probabilities to binary predictions

#Further analysis (e.g. classification report, confusion matrix)
from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.show()

# prompt: use pickle now for deoploying in flask

# import pickle

# # Assuming 'rf_model' is your trained RandomForestClassifier
# # Save the model to a file
# with open('rf_model.pkl', 'wb') as file:
#     pickle.dump(rf_model, file)



# # with open('logreg_model.pkl', 'wb') as file:
# #     pickle.dump(logreg_model, file)

# # with open('svm_model.pkl', 'wb') as file:
# #     pickle.dump(svm_model, file)
# # with open('model.pkl', 'wb') as file:
# #     pickle.dump(model, file)

import pickle
import os
# open a file, where you want to store the data
file = open('lg_model_5.pkl', 'wb')


# dump information to that file
pickle.dump(logreg_model, file)

file = open('rf_model_5.pkl', 'wb')
pickle.dump(rf_model, file)

file = open('svm_model_5.pkl', 'wb')
pickle.dump(svm_model, file)

file = open('model_5.pkl', 'wb')
pickle.dump(model, file)